{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg, sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_clean_df(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        x = f.read()\n",
    "    lines = x.replace('\\n', ' ')\n",
    "    lines = x.replace('\\n', ' ')\n",
    "    patt = r'\\d+/\\d+/\\d+.*?(?=\\s*\\b\\d+/\\d+/\\d+|)(?=\\s*\\|)+'\n",
    "    patt2 = r'(\\d+/\\d+/\\d+.*?(?=\\s*\\b\\d+/\\d+/\\d+|)(?=\\s*\\|)+)'\n",
    "    lines2 = re.split(patt, lines)[1:]\n",
    "    full_lines = [re.findall(patt2, lines), lines2]\n",
    "    full_lines = [''.join(i).strip() for i in zip(*full_lines)]\n",
    "    df = pd.DataFrame([l.split(\"||\") for l in full_lines], columns=['date', 'type', 'name', 'post', 'delete'])\n",
    "    df = df.drop('delete', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians = ['jacindaardern', 'KelvinDavisLabour', 'simonjbridges', 'paulabennettUH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame()\n",
    "for poli in politicians:\n",
    "    df = read_and_clean_df('./data/'+poli+'/' + poli+'_posts.txt')\n",
    "    df['politician'] = poli\n",
    "    full_df = full_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove blank posts\n",
    "full_df = full_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>post</th>\n",
       "      <th>politician</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07/11/2019, 16:27</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>A pretty historic moment - today we pass our ...</td>\n",
       "      <td>jacindaardern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28/11/2019, 23:03</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>Forty years ago today 257 people lost their l...</td>\n",
       "      <td>jacindaardern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27/11/2019, 17:57</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>Mixed emotions today as we opened the new Sui...</td>\n",
       "      <td>jacindaardern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21/11/2019, 17:04</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>Despite coming from a police family (my dad w...</td>\n",
       "      <td>jacindaardern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18/11/2019, 16:02</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern was live.</td>\n",
       "      <td>#LIVE: Post-Cabinet Press Conference 18 Novem...</td>\n",
       "      <td>jacindaardern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date      type                        name  \\\n",
       "0  07/11/2019, 16:27     others              Jacinda Ardern    \n",
       "1  28/11/2019, 23:03     others              Jacinda Ardern    \n",
       "2  27/11/2019, 17:57     others              Jacinda Ardern    \n",
       "3  21/11/2019, 17:04     others              Jacinda Ardern    \n",
       "4  18/11/2019, 16:02     others    Jacinda Ardern was live.    \n",
       "\n",
       "                                                post     politician  \n",
       "0   A pretty historic moment - today we pass our ...  jacindaardern  \n",
       "1   Forty years ago today 257 people lost their l...  jacindaardern  \n",
       "2   Mixed emotions today as we opened the new Sui...  jacindaardern  \n",
       "3   Despite coming from a police family (my dad w...  jacindaardern  \n",
       "4   #LIVE: Post-Cabinet Press Conference 18 Novem...  jacindaardern  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.savetxt('input.txt', full_df['post'].values)\n",
    "# with open('input.txt', 'a') as f:\n",
    "#     f.write(full_df['post'].to_string(header = False, index = False))\n",
    "# full_df['post'].to_string(header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2379, 6855)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = vectorizer.fit_transform(full_df['post']).todense()\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '01st', '035', '10', '100', '1000', '1000s', '1000th',\n",
       "       '100kg', '100m'], dtype='<U26')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vh = linalg.svd(vectors, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics, num_top_words = 3, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cray whuuu downwiththeyouth dodgy hoaket',\n",
       " 'new government tax kiwis zealand',\n",
       " 'government today tax national labour',\n",
       " 'tourism government nz tax infrastructure',\n",
       " 'today great national nz zealand']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(Vh[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_topic_model_svd(politicians=[]):\n",
    "    df = full_df[full_df['politician'].isin(politicians)]\n",
    "    vectors = vectorizer.fit_transform(df['post']).todense()\n",
    "    U, s, Vh = linalg.svd(vectors, full_matrices=False)\n",
    "    return show_topics(Vh[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['500 brilliant bad coastlines atatu',\n",
       " 'chair diverse dealt declares bosses',\n",
       " 'chair alternativefacts dominion 31kg 8billion',\n",
       " 'alternativefacts declares 8billion biennial caution',\n",
       " 'declares diverse debt dogs dealt']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_svd(['jacindaardern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abused hoopin break beeram asset',\n",
       " 'intervention 7pm installation face kimmel',\n",
       " 'intervention inn installation commerce chooks',\n",
       " 'creatures ladies chooks commerce accomplishment',\n",
       " 'inn 7pm chooks experienced bilateral']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_svd(['paulabennettUH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gotta 39 controlled diversifying 1m',\n",
       " 'flood 330km daughter crowd check',\n",
       " 'foreign 330km gloooooowing decrease crowd',\n",
       " '330km daughter folau cleared crowd',\n",
       " 'foreign flood dealt cleared cookies']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_svd(['simonjbridges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affairs buyers me clean pastoral',\n",
       " 'ngarimu parental home needed achievements',\n",
       " 'achievements energy heretaunga handling perimeter',\n",
       " 'ngarimu energy damn natural perimeter',\n",
       " 'energy needed perimeter multi developed']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_svd(['simonjbridges', 'paulabennettUH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mountain guardianship passionately handwritten mess',\n",
       " 'mess exhibitors gun meadowood fabulous',\n",
       " 'broadwood money homes papanui handwritten',\n",
       " 'mountain handwritten broadwood golf defending',\n",
       " 'capped money mountain particularly homes']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_svd(['jacindaardern', 'KelvinDavisLabour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(full_df['post']) # (documents, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 7\n",
    "clf = decomposition.NMF(n_components=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new national zealand party document',\n",
       " 'live press conference post cabinet',\n",
       " 'great today day mp people',\n",
       " 'crown holding minister awesome designing',\n",
       " 'te reo hui tai tokerau',\n",
       " 'tourism nz infrastructure industry new',\n",
       " 'tax government labour kiwis car']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_topic_model_nmf(politicians=[]):\n",
    "    df = full_df[full_df['politician'].isin(politicians)]\n",
    "    vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "    vectors_tfidf = vectorizer_tfidf.fit_transform(df['post']) # (documents, vocab)\n",
    "    clf = decomposition.NMF(n_components=num_topics)\n",
    "    W1 = clf.fit_transform(vectors_tfidf)\n",
    "    H1 = clf.components_\n",
    "    return show_topics(H1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9m club coincidental ago built',\n",
       " 'chair dog circumstances dominion dollar',\n",
       " 'commission depths disappointed documentary brings',\n",
       " '35 deal announced 10s cpnz',\n",
       " '2250 declares 60th buildings 3lb',\n",
       " '8billion cracks alternativefacts dinners 550',\n",
       " '136 built coincidental ago 32m']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_nmf(['jacindaardern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['colossal informal fairer dumped inclusiveness',\n",
       " 'ends knocked festival inpatient ford',\n",
       " 'employed extra hayes allegations impressive',\n",
       " 'emails discovered eli current braved',\n",
       " 'impersonate dependent colbert delivery aotea',\n",
       " 'bikes kickoff culture competed details',\n",
       " 'fox keyte ceo episode coffee']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_and_topic_model_nmf(['paulabennettUH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mixed', 'emotions', 'today', 'as', 'we', 'opened', 'the', 'new', 'Suicide', 'Prevention', 'Office', '.', 'There', '’', 's', 'so', 'much', 'support', 'for', 'the', 'work', 'to', 'end', 'suicide', 'in', 'New', 'Zealand', '-', 'people', 'like', 'Nehe', 'Milner-Skudder', 'who', 'is', 'working', 'in', 'mental', 'health', 'and', 'well-being', 'with', '@', 'headfirstnz', '.', 'All', 'of', 'us', 'want', 'a', 'country', 'where', 'an', 'office', 'like', 'this', 'isn', '’', 't', 'needed', ',', 'till', 'then', 'I', '’', 'm', 'grateful', 'for', 'all', 'the', 'people', 'working', 'to', 'make', 'that', 'world', 'possible', '(', 'like', 'Sir', 'Mason', 'Durie', 'who', 'has', 'kindly', 'agreed', 'to', 'chair', 'our', 'Maori', 'Advisory', 'Board', 'and', 'keep', 'sharing', 'all', 'his', 'wisdom', '.', ')']\n",
      "Positive : ['wisdom']\n",
      "Neutral : ['Mixed', 'emotions', 'today', 'as', 'we', 'opened', 'the', 'new', 'Prevention', 'Office', '.', 'There', '’', 's', 'so', 'much', 'support', 'for', 'the', 'work', 'to', 'end', 'in', 'New', 'Zealand', '-', 'people', 'like', 'Nehe', 'Milner-Skudder', 'who', 'is', 'working', 'in', 'mental', 'health', 'and', 'well-being', 'with', '@', 'headfirstnz', '.', 'All', 'of', 'us', 'want', 'a', 'country', 'where', 'an', 'office', 'like', 'this', 'isn', '’', 't', 'needed', ',', 'till', 'then', 'I', '’', 'm', 'grateful', 'for', 'all', 'the', 'people', 'working', 'to', 'make', 'that', 'world', 'possible', '(', 'like', 'Sir', 'Mason', 'Durie', 'who', 'has', 'kindly', 'agreed', 'to', 'chair', 'our', 'Maori', 'Advisory', 'Board', 'and', 'keep', 'sharing', 'all', 'his', '.', ')']\n",
      "Negative : ['Suicide', 'suicide']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text=nltk.word_tokenize(full_df['post'][2])\n",
    "print(tokenized_text)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "for word in tokenized_text:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.5:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.5:\n",
    "        neg_word_list.append(word)\n",
    "    else:\n",
    "        neu_word_list.append(word)                \n",
    "\n",
    "print('Positive :',pos_word_list)        \n",
    "print('Neutral :',neu_word_list)    \n",
    "print('Negative :',neg_word_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores('great')['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3958"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "def sentiment_analyser(text):\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    sentiment_score = 0.\n",
    "    for word in tokenized_text:\n",
    "        sentiment_score = sentiment_score + sid.polarity_scores(word)['compound']\n",
    "    return sentiment_score\n",
    "sentiment_analyser(full_df['post'][2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['sentiment_score'] = full_df['post'].apply(lambda x: sentiment_analyser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_negative_posts = full_df.groupby('politician')['sentiment_score'].min().rename('sentiment_score').reset_index()\n",
    "most_positive_posts = full_df.groupby('politician')['sentiment_score'].max().rename('sentiment_score').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>post</th>\n",
       "      <th>politician</th>\n",
       "      <th>post_clean</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13/05/2019, 11:37</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>In the wake of the 15th of March, we learned ...</td>\n",
       "      <td>jacindaardern</td>\n",
       "      <td>In wake 15th March learned happened Christchur...</td>\n",
       "      <td>-3.3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20/03/2019, 21:12</td>\n",
       "      <td>others</td>\n",
       "      <td>Kelvin Davis</td>\n",
       "      <td>Last Friday this country experienced an unpre...</td>\n",
       "      <td>KelvinDavisLabour</td>\n",
       "      <td>Last Friday country experienced unprecedented ...</td>\n",
       "      <td>-3.5290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07/10/2019, 18:14</td>\n",
       "      <td>others</td>\n",
       "      <td>Simon Bridges</td>\n",
       "      <td>Gang numbers are up 26% under this soft-on-cr...</td>\n",
       "      <td>simonjbridges</td>\n",
       "      <td>Gang numbers 26 soft on crime Government Befor...</td>\n",
       "      <td>-4.3180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29/11/2019, 07:30</td>\n",
       "      <td>others</td>\n",
       "      <td>Paula Bennett</td>\n",
       "      <td>Gang patches and insignia are intimidating. W...</td>\n",
       "      <td>paulabennettUH</td>\n",
       "      <td>Gang patches insignia intimidating We believe ...</td>\n",
       "      <td>-2.8632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date      type              name  \\\n",
       "0  13/05/2019, 11:37     others    Jacinda Ardern    \n",
       "1  20/03/2019, 21:12     others      Kelvin Davis    \n",
       "2  07/10/2019, 18:14     others     Simon Bridges    \n",
       "3  29/11/2019, 07:30     others     Paula Bennett    \n",
       "\n",
       "                                                post         politician  \\\n",
       "0   In the wake of the 15th of March, we learned ...      jacindaardern   \n",
       "1   Last Friday this country experienced an unpre...  KelvinDavisLabour   \n",
       "2   Gang numbers are up 26% under this soft-on-cr...      simonjbridges   \n",
       "3   Gang patches and insignia are intimidating. W...     paulabennettUH   \n",
       "\n",
       "                                          post_clean  sentiment_score  \n",
       "0  In wake 15th March learned happened Christchur...          -3.3700  \n",
       "1  Last Friday country experienced unprecedented ...          -3.5290  \n",
       "2  Gang numbers 26 soft on crime Government Befor...          -4.3180  \n",
       "3  Gang patches insignia intimidating We believe ...          -2.8632  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.merge(most_negative_posts, how='inner', on=['politician','sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>post</th>\n",
       "      <th>politician</th>\n",
       "      <th>post_clean</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19/10/2019, 12:37</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>I can’t say I’ve ever said this before - but ...</td>\n",
       "      <td>jacindaardern</td>\n",
       "      <td>I can t say I ve ever said Nelson beautiful ai...</td>\n",
       "      <td>5.8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21/06/2019, 12:54</td>\n",
       "      <td>others</td>\n",
       "      <td>Kelvin Davis is in Queenstown, New Zealand.</td>\n",
       "      <td>In Queenstown yesterday to help open the Wint...</td>\n",
       "      <td>KelvinDavisLabour</td>\n",
       "      <td>In Queenstown yesterday help open Winter Festi...</td>\n",
       "      <td>5.9465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25/06/2019, 16:28</td>\n",
       "      <td>others</td>\n",
       "      <td>Simon Bridges</td>\n",
       "      <td>This morning my friend and colleague Amy Adam...</td>\n",
       "      <td>simonjbridges</td>\n",
       "      <td>This morning friend colleague Amy Adams MP tol...</td>\n",
       "      <td>4.6093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30/08/2017, 09:53</td>\n",
       "      <td>others</td>\n",
       "      <td>Paula Bennett</td>\n",
       "      <td>Over 200 delegates are in Dunedin for the Hol...</td>\n",
       "      <td>paulabennettUH</td>\n",
       "      <td>Over 200 delegates Dunedin Holiday Parks Confe...</td>\n",
       "      <td>4.8395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date      type  \\\n",
       "0  19/10/2019, 12:37     others    \n",
       "1  21/06/2019, 12:54     others    \n",
       "2  25/06/2019, 16:28     others    \n",
       "3  30/08/2017, 09:53     others    \n",
       "\n",
       "                                            name  \\\n",
       "0                                Jacinda Ardern    \n",
       "1   Kelvin Davis is in Queenstown, New Zealand.    \n",
       "2                                 Simon Bridges    \n",
       "3                                 Paula Bennett    \n",
       "\n",
       "                                                post         politician  \\\n",
       "0   I can’t say I’ve ever said this before - but ...      jacindaardern   \n",
       "1   In Queenstown yesterday to help open the Wint...  KelvinDavisLabour   \n",
       "2   This morning my friend and colleague Amy Adam...      simonjbridges   \n",
       "3   Over 200 delegates are in Dunedin for the Hol...     paulabennettUH   \n",
       "\n",
       "                                          post_clean  sentiment_score  \n",
       "0  I can t say I ve ever said Nelson beautiful ai...           5.8033  \n",
       "1  In Queenstown yesterday help open Winter Festi...           5.9465  \n",
       "2  This morning friend colleague Amy Adams MP tol...           4.6093  \n",
       "3  Over 200 delegates Dunedin Holiday Parks Confe...           4.8395  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.merge(most_positive_posts, how='inner', on=['politician','sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>post</th>\n",
       "      <th>politician</th>\n",
       "      <th>post_clean</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13/05/2019, 11:37</td>\n",
       "      <td>others</td>\n",
       "      <td>Jacinda Ardern</td>\n",
       "      <td>In the wake of the 15th of March, we learned ...</td>\n",
       "      <td>jacindaardern</td>\n",
       "      <td>In wake 15th March learned happened Christchur...</td>\n",
       "      <td>-3.3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20/03/2019, 21:12</td>\n",
       "      <td>others</td>\n",
       "      <td>Kelvin Davis</td>\n",
       "      <td>Last Friday this country experienced an unpre...</td>\n",
       "      <td>KelvinDavisLabour</td>\n",
       "      <td>Last Friday country experienced unprecedented ...</td>\n",
       "      <td>-3.5290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07/10/2019, 18:14</td>\n",
       "      <td>others</td>\n",
       "      <td>Simon Bridges</td>\n",
       "      <td>Gang numbers are up 26% under this soft-on-cr...</td>\n",
       "      <td>simonjbridges</td>\n",
       "      <td>Gang numbers 26 soft on crime Government Befor...</td>\n",
       "      <td>-4.3180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29/11/2019, 07:30</td>\n",
       "      <td>others</td>\n",
       "      <td>Paula Bennett</td>\n",
       "      <td>Gang patches and insignia are intimidating. W...</td>\n",
       "      <td>paulabennettUH</td>\n",
       "      <td>Gang patches insignia intimidating We believe ...</td>\n",
       "      <td>-2.8632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date      type              name  \\\n",
       "0  13/05/2019, 11:37     others    Jacinda Ardern    \n",
       "1  20/03/2019, 21:12     others      Kelvin Davis    \n",
       "2  07/10/2019, 18:14     others     Simon Bridges    \n",
       "3  29/11/2019, 07:30     others     Paula Bennett    \n",
       "\n",
       "                                                post         politician  \\\n",
       "0   In the wake of the 15th of March, we learned ...      jacindaardern   \n",
       "1   Last Friday this country experienced an unpre...  KelvinDavisLabour   \n",
       "2   Gang numbers are up 26% under this soft-on-cr...      simonjbridges   \n",
       "3   Gang patches and insignia are intimidating. W...     paulabennettUH   \n",
       "\n",
       "                                          post_clean  sentiment_score  \n",
       "0  In wake 15th March learned happened Christchur...          -3.3700  \n",
       "1  Last Friday country experienced unprecedented ...          -3.5290  \n",
       "2  Gang numbers 26 soft on crime Government Befor...          -4.3180  \n",
       "3  Gang patches insignia intimidating We believe ...          -2.8632  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.merge(most_negative_posts, how='inner', on=['politician','sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kssdfsdfdsdfsdffsdfsdfssdfsdfdl'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [len(word) for word in nltk.word_tokenize('dfgd dfgd')]\n",
    "#     print(word)\n",
    "def remove_one_letter_words(text):\n",
    "    text_clean = ''\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if len(word)>1:\n",
    "            text_clean = text_clean.join(word)\n",
    "    return text_clean\n",
    "remove_one_letter_words('sdfsdf sdfsd g kl')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "full_df['post_clean'] = full_df['post'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "full_df['post_clean'] = full_df['post_clean'].apply(lambda x: ' '.join(word for word in reg_tokenizer.tokenize(x)))\n",
    "\n",
    "#remove one letter words\n",
    "full_df['post_clean'] = full_df['post_clean'].apply(lambda x: remove_one_letter_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'post_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2655\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2656\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2657\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'post_clean'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c052fc9b1b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtop_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#if not necessary all lower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'post_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mword_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'post_clean'"
     ]
    }
   ],
   "source": [
    "top_N = 14\n",
    "#if not necessary all lower\n",
    "a = full_df['post_clean'].str.lower().str.cat(sep=' ')\n",
    "words = nltk.tokenize.word_tokenize(a)\n",
    "word_dist = nltk.FreqDist(words)\n",
    "print (word_dist)\n",
    "# <FreqDist with 17 samples and 20 outcomes>\n",
    "\n",
    "rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
