{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.learner import *\n",
    "from fastai.basic_data import DataBunch\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.getcwd()\n",
    "# os.chdir('drive/My Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data I/O\n",
    "data = open('input_bible.txt', 'r').read() # should be simple plain text file\n",
    "words = Counter(data.split())\n",
    "words = [k for k in words.keys()]\n",
    "data_size, vocab_size = len(data), len(words)\n",
    "word_to_ix  = { ch:i for i,ch in enumerate(words) }\n",
    "ix_to_word = { i:ch for i,ch in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31402"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#create a custom data dataset / dataloader\n",
    "class bible_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, seq_len):\n",
    "        self.df = open('input_bible.txt', 'r').read().split()\n",
    "        self.vocab = Counter(self.df)\n",
    "        self.vocab = [k for k in self.vocab.keys()]\n",
    "        self.word_to_ix  = {wr:i for i,wr in enumerate(words)}\n",
    "        self.ix_to_word = {i:wr for i,wr in enumerate(words)}        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (np.array([self.word_to_ix[wrd] for wrd in self.df[idx:idx+self.seq_len]]),\n",
    "                np.array([self.word_to_ix[wrd] for wrd in self.df[idx+1:idx+self.seq_len+1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 2, 6, 7, 2, 8, 9]),\n",
       " array([ 3,  4,  5,  2,  6,  7,  2,  8,  9, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_df = bible_dataset(10)\n",
    "bible_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a vanilla rnn \n",
    "\n",
    "class vanilla_rnn(nn.Module):\n",
    "    def __init__(self, input_size, output_size, \n",
    "                 hidden_dim, layers):\n",
    "        super(vanilla_rnn, self).__init__()\n",
    "        \n",
    "        self.size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size=hidden_dim, num_layers = layers,\n",
    "                      batch_first=True)\n",
    "        #a fully connectd layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    #initialise the hidden states before training\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.layers, batch_size, \n",
    "                            self.hidden_dim)\n",
    "        return hidden\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, x):\n",
    "#         print(x)\n",
    "        batch_size = x.shape[0]\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "\n",
    "bible_df = DataLoader(bible_dataset(seq_len), 1)\n",
    "# bible_df\n",
    "\n",
    "v_rnn = vanilla_rnn(input_size=vocab_size,\n",
    "                    output_size=vocab_size,\n",
    "                    hidden_dim=12,\n",
    "                    layers=1)\n",
    "n_epochs = 1\n",
    "lr=1e-1\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(v_rnn.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode over the whole vocab\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class CyclicLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "    \n",
    "def cosine(t_max, eta_min=0):  \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-19f6c89f701f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutput1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# loss_func(input=output, target=target_seq.transpose(0,1).long().squeeze(1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_seq' is not defined"
     ]
    }
   ],
   "source": [
    "torch.from_numpy(one_hot_encode(input_seq, vocab_size, 10, 1)).unsqueeze(1).view(-1).long().shape\n",
    "output1, hidden1 = v_rnn(torch.from_numpy(one_hot_encode(input_seq, vocab_size, 10, 1)))\n",
    "output1.shape,target_seq.transpose(0,1).long().squeeze(1).shape\n",
    "# loss_func(input=output, target=target_seq.transpose(0,1).long().squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elements of output are of size = vocab\n",
    "#there are 10 elements (size of batch/sentence/sequence)\n",
    "# output[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_seq.size(), output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_seq.transpose(0,1).long().squeeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "\n",
    "bible_df = DataLoader(bible_dataset(seq_len), 1)\n",
    "# bible_df\n",
    "\n",
    "v_rnn = vanilla_rnn(input_size=vocab_size,\n",
    "                    output_size=vocab_size,\n",
    "                    hidden_dim=12,\n",
    "                    layers=1)\n",
    "n_epochs = 1\n",
    "lr=1e-2\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(v_rnn.parameters(), lr=lr)\n",
    "\n",
    "iterations_per_epoch = len(bible_df)\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))\n",
    "\n",
    "loss_values = []\n",
    "#training loop\n",
    "for epoch in range(1,2):\n",
    "    loss= 0.\n",
    "    for local_batch in enumerate(bible_df, 0):\n",
    "        input_seq = local_batch[1][0]\n",
    "        target_seq = local_batch[1][1]\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = v_rnn(torch.from_numpy(one_hot_encode(input_seq, vocab_size, 10, 1)))\n",
    "        loss = loss_func(input=output, target=target_seq.transpose(0,1).long().squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_values.append(loss.data.item())                        \n",
    "        if (len(loss_values) % 100 == 0) | (len(loss_values) == 1):\n",
    "            print(loss.data.item())\n",
    "            loss_values.append(loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.lines.Line2D at 0x172e87edf28>], 31720)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXd4FWX2x78nIRBK6KFJCU2qSIkIIr1Isa4NFju7WFdRfyqWVSworh1REAW7iHXVBYGAKCBIlQ4hAYL0UEMNEHh/f9yZZO7c6eXW83mePLl3Zu7Mmfadd857znlJCAGGYRgmcUiKtAEMwzBMeGHhZxiGSTBY+BmGYRIMFn6GYZgEg4WfYRgmwWDhZxiGSTBY+BmGYRIMFn6GYZgEg4WfYRgmwSgVaQO0qF69usjIyIi0GQzDMDHD8uXL9wsh0q0sG5XCn5GRgWXLlkXaDIZhmJiBiLZZXZZdPQzDMAkGCz/DMEyCwcLPMAyTYLDwMwzDJBgs/AzDMAkGCz/DMEyCwcLPMAyTYLDwMwwT18xctwf7jp6KtBlRBQs/wzBxy8nTZ3Hnp8tx86TFkTYlqmDhZxgmbjkrBABg+8ETEbYkumDhZxgm7hGRNiDKYOFnGCZuIem/YOUPgoWfYZi4hch8mUSEhZ9hGCbBYOFnGIZJMFj4GYaJewR37wbBws8wTNxCYCe/Fiz8DMPEPRzVEwwLP8MwcYsc1cO6HwwLP8MwTIJhOtg6EU0GcDmAfCFEa2naVADNpEUqAzgshGir8ds8AEcBnAVQJITI9MhuhmEYxiGmwg/gIwDjAHwiTxBC3Ch/JqLXABQY/L6nEGK/UwMZhmEYbzEVfiHEPCLK0JpHRATgBgC9vDWLYRjGQ9jJH4RbH39XAHuFEDk68wWAWUS0nIiGu9wWwzAM4wFWXD1GDAEwxWB+FyHELiKqASCLiDYKIeZpLSg9GIYDQP369V2axTAMUwIncAXjuMVPRKUA/A3AVL1lhBC7pP/5AL4H0NFg2YlCiEwhRGZ6erpTsxiGYYrh+H1t3Lh6+gDYKITYoTWTiMoTUZr8GUA/AGtdbI9hGIbxAFPhJ6IpABYBaEZEO4homDRrMFRuHiKqQ0TTpa81ASwgolUAlgCYJoSY4Z3pDMMw1uCWfzBWonqG6Ey/TWPaLgADpc9bAFzo0j6GYRjGYzhzl2GYuIcb/MGw8DMMwyQYLPwMw8QtHMapDQs/wzBMgsHCzzBM3CM4rCcIFn6GYeIelv1gWPgZhmESDBZ+hmHiFvbwaMPCzzAMk2Cw8DMME/dwyz8YFn6GYZgEg4WfYRgmwWDhZxiGSTBY+BmGiVvYta8NCz/D+Mi2A8eRMXIaZq/fG2lTGKYYFn6G8ZGV2w8DAH5YtSvCljBMCSz8DMMwCQYLP8MwTIJhZczdyUSUT0RrFdNGEdFOIlop/Q3U+W1/IsomolwiGuml4QzDMIwzrLT4PwLQX2P6G0KIttLfdPVMIkoG8A6AAQBaAhhCRC3dGMswDGMHLsesjanwCyHmATjoYN0dAeQKIbYIIU4D+BLAVQ7WwzAMw3iIGx//fUS0WnIFVdGYfx6A7YrvO6RpDMMwTARxKvzjATQG0BbAbgCvaSxDGtN037uIaDgRLSOiZfv27XNoFsMwDGOGI+EXQuwVQpwVQpwD8D4Cbh01OwDUU3yvC0A3mFkIMVEIkSmEyExPT3diFsMwTBBTlvwVaROiEkfCT0S1FV+vAbBWY7GlAJoSUUMiKg1gMIAfnWyPYRjGCS9O3xhpE6ISK+GcUwAsAtCMiHYQ0TAA/yGiNUS0GkBPAA9Ky9YhoukAIIQoAnAfgJkANgD4Sgixzqf9YBiGiUoe+2Y1MkZOi7QZQZQyW0AIMURj8iSdZXcBGKj4Ph1ASKgnwzCM30RLKOfUZdvNFwoznLnLMExYWZCzH22fm4Vjp4oibUrCwsLPMExYeWVWNg6fOIOcvUd93U6UNPijEhZ+hmHCS5gUmXVfHxZ+hmEiApFWqk8ox08V4R8fL8XugpM+W5Q4sPAzDBNW7LbEp63Zjdkb8vHarE2+2JOIsPAzDBMRrLX3nRMtUT3RCAs/w/jI6aJzAICjhWcibEniwbKvDws/w/jILGms3V+zuf6UjO2GOCu457DwM4yP9G1REwBweZvaJksmDkJScot9u8XYdQ2pHzAZI6dh+bZDNtcSn7DwM4yfSGpVplRyZO2IA7xo+P+ane/BWmIfFn6GCQN2W7d2uXb8QnwdhaUBIsWegkLsOszhn3qY1uphGMYFYfJPL992CMu3HcL1mfXMF44SyKbzxs7SnV6a43od8Qy3+BkmDLDglMBRlpGHhZ9hfERwSIoufru/omej0QcLP8OEAdYb55g9PHPzj2LtzoIwWRMfsPAzDBNWvHb19Hl9Hi5/e4Fn6xs7JwezpfyLeIU7dxmGiWrsdgIbr8uc17MCNYHyxgzybLvRBrf4GcZH1K3be79YgRlrd/u2vavGLcAL/1vv2/q9wH7iLveTeI2VMXcnE1E+Ea1VTHuFiDYS0Woi+p6IKuv8Nk8am3clES3z0nCGiSXkVuu01btx12crfNvOqh0F+GDBVsNlHpq6Ei/PiPwg5LYzdz1o+HNfSwArLf6PAPRXTcsC0FoI0QbAJgCPG/y+pxCirRAi05mJDJN4rN5xGPuOnvJl3d/9uRPjf93sy7qZ2MBU+IUQ8wAcVE2bJYSQB8z8A0BdH2xjopi8/cfx6sxsLn1rgtOjc+W433HZm/M8tUWPhbn7kTFymmFkjBACh46f9mR7Tq8ZvtS8wwsf/x0AftaZJwDMIqLlRDTcg20xPrBk60G0fmYmCk5YLx18x0dLMW5uLnYcSoy0+O6vzMXj360O6zYPeiS0ZszeEKhf88eWA7rLTP49D+2ez8K2A8c9267tzN04ctPsLjiJorPnIrZ9V8JPRE8CKALwuc4iXYQQ7QEMAHAvEXUzWNdwIlpGRMv27eMStuHk7V9ycOxUEf7cbr1y4akiaxftydNn0ef137As76D5wmHkh5U7sX7XEcvLbztwAlOWWK+Fs3zbQSzI2e/EtKhk7sbAw+GvgycibIkxefuPo8crc3Xnyw+bl6ZviFjBtn1HT6HzS79gzM8l/SxfLd2O5dvCd484Fn4iuhXA5QCGCp13NyHELul/PoDvAXTUW58QYqIQIlMIkZmenu7ULMYB8tinfrxJr999BLn5xzB6+gYf1u6cB75ciYFj5/u2/mvHL8JNkxYXf4+X1mo0uFvemZuLGyYs0pz34e9bkXfA/OH03rwtuO3DpV6bZolDJwJvcr9tKmngPvrtalw7Xnuf/MCR8BNRfwCPAbhSCKF5lImoPBGlyZ8B9AOwVmtZJrIUa1IU3NTxRjiEcuFm/98s/HhwWV2nfAwPHDuN/COFeGVmNpY4fIP8culflpc9e07g3Ln4vCmshHNOAbAIQDMi2kFEwwCMA5AGIEsK1ZwgLVuHiKZLP60JYAERrQKwBMA0IcQMX/aCcUWSdANyvLR/+Nni/+yPbf6t3AecPgznbMxHxxe1q27KmLX2dxcUWt5e4yem44px3mUERxOmmbtCiCEakyfpLLsLwEDp8xYAF7qyjgkLxa6eBNZ9IQSuHb8Qw7s1Qv/W+qNlrdtVgFZ1Kllfr8uHaf6RQtSomGq8DZfnzcpD6dipIvOFfNiuneWAYPeJF6yz0Q8US3DmLlOMEwGJF9/1OQGs+Osw7vncOLnqDSmd3z7ODtR1Or5sLzE77/lHC/HnX4d9t0OPaGiQ/LRqF3Lzj0baDM9g4WeKJSkK7q+EZP2uI/g9V9tP/9fBE/jvnztx2mIUlR8s3lLiT/fiGokml+IL/1uPjXsCrfoFOfvx1VLtyK1/TfkTfV73Jq8iGh5kLPxMcaudk7Eiw8Cx8zH0g8W680dMXYm3f8nRnW/ntB0+EZobEKm3Ni+LrznlgwVbMfT9wLG/adJiPPqtca5GvAznyMLPOArnjOWHhDJx5qWfNwTti9le2d1tefkpS/4KKvW7/9gpjPslx/Jx9Kp8w54j+p2bMXxKAQBdxvzi+zaWbI2ufBSnsPAzJa4eRz7+yLfa7DI3u6QD8L3ftiB7r3XfrRtt/McnJXUKH/5qFV6dtQkrIug7d8vJ02eLyzjkHy3EgWP2Hk5WXD7zc/Zhxro9lta302FrPFzX8MnTZwEAm6Tr7UCYMrO1YOFnFPjR5POvGVl09hzGzsnBidP2Ik6MUuW9lgC9vZejZM5ZfNp+uXS7JzHlTtwrero4aOx8tHs+CwDQcfQcdHhhtuc23DxpCX7N9jeT30z3M0ZOK/7spn/iuf+tAxDoLwDCV5JDCxZ+RuHjt/6baPAKfPfnTryetQmvz7IXaaO23Y4Q2XZx6Swvr8eODB/S8M8D7jtL7dig3P8t+wN1e9bssDfsoZ03LD9QP/iTbByAA8eci3U01bVi4U8w3p+3Bd1VtUySHPj45daK+T1TkiMwa90eW63WE6eLDIVWjnQ5ceas5XXKtgRZqNgJLx5ony7KK/68eZ92UTN5Oyv+sl4fyQkLN+83rRnkdp+VSU6TTMYCiAaaPBlcU9LOg/9LnagfAHhrdk6xGyfaYeFPMEZP34BtquxGoxa/3uuoXKTtrEUhX7n9MIZ/uhyfL7GWMp+3/zhaPj0TUw1uNCdvKprrgfWWvJWlPl9svSzAi9P9HRDl7+8vDmvNoOdtjP4VLZ3JXhyTE6eL8MbsTbh2/ELdZebn7McvG6NjLF8W/hjj7DmBN2dvwtFC6yWUrfLTql1B35dsPYj2z2d5OlTgXosp87n5xwAAI79bg6VSXZbCM2eRvUerRWVPQYxcI2Ya4JVYeSl6S/O03xqslv2Nve55b/Fi/+XzWXQ2+MQeUd2nd3wUHQMRsvBL/JqdHxNldKev2Y03Z+d42lKcsTYQNaGOnpgsvbbP8/C4OGldzZe2P/Lb1bjszXnFkSSkcCPZQY6ucGOTEcooEb03Ca90/2jhGd23sqd/XGf424yR05CvCO/0KrFq9LT1liJsoqfF78+jb2neQbQZNcuXdbuFhV/itg+XBr0SRytnpFZcoU2/thF63hr5QZC795hn29K6xSbO24wdh4LdT1omyS3b41IUj9P7NbTujEKona1SF931eaR6Rhm9P68peVM7Lu2z+pDl5JufW7J5fN6fvxX3S5ErbvCjPpAWdq4jO5fcyigO1WXhj1PyjxSi80tzsHmfd6LthHnqolmqu2x3wUm8OH0j7vhIvza6E/eLEAIfzN9iSTyIrAu+XbnW03c3sr9p71HNDFwjWj0zEzkmHY9eZtIq3UwfzN8S8mC3QutnZnpmjxFWBxUCzM9brKS1sPDHKdPX7MbugkJ8sjDP/cqki/l00bmQ6ofqC/3g8dMYPW198Y3/1pzgUgPqkbjklujxU8FvMEadrfIso3vst0378MK0DXhW5e4QQoT4YUnnMxBITLp18hJLdumtQws3Df5+b8zDVe/8br4N1XetMEplqQgva+iskkI89x4pxAvTNuB2jUFP3G7Pq/pFdrKiled/637vhqEMNyz8McTanQW2WieAN64LeYjC17KycevkJUFp6zdMWBRU2OrZn9bh/flbkbVeO3ph4eYDWJZ3sPhN5OGvA7VR9tvM+lSiJSCyK+zr5TuCpo/5eaOt0cBGfLnSValfPXFzK3rbDpzA0ryDmL5WP6s1JGzVYYveSWtdRo768sNto+44DScz1u5Gz1d/xSzJHao81EIIvDM3FwdtvpWFE9N6/Ex0sKegEJe/vQClLGabeNlhJd+02/YHBECZmr+roBCPfrsaN1xUD2t3FuCHlYHIIKMoT7nUcN6YQVi9I+AHVT/QDp8suan1dsUonFOvRf2FRqhlUGesat5xn/zMXrj4rzcp2ax+O9E7jtslYd+oGTEVmnj02qxszNlgbbxa2YJzQmBBzn50aVJNYZ+lVUQV8rUi1+nfuOco+rWqFbTMir8O4ZWZ2WG3zQ7c4rfB23NycK9JvXYl78zNxfvztpj6YwtOnMH4XzcbuhEOnwysoyiCQ8HJwqFlQsHJM7j87QUhy5qht8uPfmNcJREoORb7XLwtAEDPV3/FCxrx5wePn3bULrey705ET+8tyg5ats1cF1jvdyt2Fg+qruRT5QhfAnj7l1ys321tgBK5muXeI6dw06TFmOXBPshEgzs99K0KOHM2+p9oLPw2eC1rE6at2Y1Fmw9YykB9ZWY2Rk/fgL5vGNfxfuqHtXh5xkY0fHy67jLnHLozvWhVyWJR3MKGQNXypYOWWbzlgOvt2K1Aeaww0BrPsRN1pKMWHy8KHr4wa/1etH8+C6tV5QjmG4S27jx8MqQz/dBx6+4Is/0f+d0ay+sCQt9erLwsqu13Ez124nRRyFvJXoPqoHaJhMCqS21oueyi4YFkRlwJv9ZFKoTA+/O2FMd+e8GQ9/9Aoyf0RVqN2Q19TOWr3Hf0VMgNqOcTnrF2D/aYJEUJITB7/V7LWbZq5JIOdoZodHLxXzR6NvKPFIZkN+r5puWHz8WNqjrYmjbylubnmPv1C8+cxWuzsnGqKHDddRnzC3q/9lvQMtPWaCe/aR3Ci0abFzmzmmG8cvvhkI3c9dkK2w2BWyYtMV9IB3WHPeCte6fTS3Pw/Z87zBf0A9Wrk/K8xELFWkvCT0STiSifiNYqplUloiwiypH+V9H57a3SMjlEdKtXhmvR/N8z8LEqimXFX4cxevoGPPLNKs+351dN+kvGzAkREL1N3fXZcsM0cQCYuW4P/vHJMtz12fLiaWPn6A/soUa+jOUHwDkhfGvVjPppnWl24xXjFqDnq78q7CuxpuDEGeTmH3Pcdar0SZsx4bfNePuXXHyqeluwwpGTzjomrb4VXf3O7ziq0T9h97gsyYvu+vMPTvX+vraD1mViV/ftlrP2Aqst/o8A9FdNGwlgjhCiKYA50vcgiKgqgGcAXAygI4Bn9B4QXqGuQyMnPB05qd1Jd+bsOTz5vf4r9B9bDqD981maJRK+WqZfR8aMdbsKdH+v9QqrFqLv/9xZ/FkrS1J58e09EriwlD7i122MHUsEfL54G1ZtD3TECmEuIKctlgtQs0WjqNkbszfhotGzi/fz8IkzmqF0Qghc/e7v6PP6byHz/EDukFZ3TFu58Z3Wjo+09/gxkxGqlJgdh90FhXj2p3WO30QjgWzpF4sDD3u3kUV7CgrR/ZVf3RnlAEvCL4SYB0D96L8KwMfS548BXK3x08sAZAkhDgohDgHIQugDxDMqppYKEUgjXxwQSDBSFtWam50fNFLS61mbcPD46eJefCXbDzovszpo7ILiDkwrl71ekpLp7wzWnjFyGtbuNC+pS0R48vu1+Otg4KFqpTX8wJcrTZexg5WW7o3v/aH5QNjiUxKb8jD8V/EgjoYhBZ3ywjTjUNd8G/0wr80yjmy574sV+PD3PCzLO4h1uwow6sd1UT+ym9ww2S+VZ5bdrLLVRPbOfqeX5oQtQ1mJGx9/TSHEbgCQ/tfQWOY8AMpm7Q5pWghENJyIlhHRsn37nMVNJyWRRgib8WlQX2e3f7g0aKQkmZs0xkT1e9Do535aX1ygTEts5Za8Fsq9NrqZ1IXZzNalx4bd9svRfjB/S8g0N/e90i2hXM9nf1ivlulEss+cPYcRU0sedGssPEyd4lYXtYvc+cOUJaFvtM8oEurkN6Vjp4owaOwCfLQwL2I1653W6fLC1RMJ/O7c1ToEmpeuEGKiECJTCJGZnp7uaGNJRLrx405vGHkH/Ayj1LtOJv++tTgqQmvznV6aY7pu0/12eJGqC4N9u8J+J5tW69JO60e5ay+pkrLu/UI77Na0/IP03yhpa2newaDiZjHkqcC4X3IjbUIIypj3rv+ZGxEbXstyFnevbvwdO1WEa8cb51dEA26Efy8R1QYA6b9WRscOAPUU3+sCMG9iOiSJQlvGJSGI9vj7+394Y5Rn+KMuVl5Mk0IiGHwxBQBs16ABAuf4vXmhbw8yU5dab/HLGLnxrp+wCP3fml+yfdtrjxx+v6U6wW42ejQxc91e7CkoxLfLIxRd5BA3wv8jADlK51YAP2gsMxNAPyKqInXq9pOm+QJptPid3pQLNx+Q1qm/jJ4Art5xGHOzrWU2WsWsVbl53zFHJYC1YrvX7SoIGmdUfQw+XpRnaT1OOH7aetz4Jotui+Onz+L/vl7laSei8o0nnK/2boU7Gl3osVzzBgCe+u8aLNrsPo8lnFgN55wCYBGAZkS0g4iGARgDoC8R5QDoK30HEWUS0QcAIIQ4COB5AEulv+ekab6QRMH+7APHTjmOnpAxahHvO3pKMxTrynG/hxSlWr7tIApOaEcAKO/Ft2Zrh1luP2hcL6X3a7/hE0VY4b9/MK7FLqMlWoPGLgheRjVfndQEhL4VhIMPpPECftHINlXzzfIdxZ3TRtgafzYKW89mWLFYLqPBaPN7bnB/wAGHGd6RxFKtHiHEEJ1ZvTWWXQbgH4rvkwFMdmSdTQgU5Orp8EJJQowf0QJfL9+Br5fvQN6YQQACnXwPfaUdV3zt+EW4sF5l/HBvF8N1vjE7NMxy+bZDlsLo5HBLJQLGrTyt8Ek1lhJSIujv0BuIxAnRfgN3fukXV7/PtVB/f+rS7WhTt7Kr7bjh8InTqFyutPmCEWKoKtDjz78Oo0+LmhGyxhlxlbkbaPGbL1d45iy+Wrrd84fB+l1HDKNk1ui0pMw089rxCy2lp/+0WnvbRr/8WVXdsfm/fw5Zxoqmx4Kfe++RQl+yKmcYVMiMRdRlq8NN2+eyUOAwwc0pf7oeNCXamwzBxFV1TtnH3+0/c0NqdStPy+tZmzBx3hYUnRN4wiB5K7BO7+zTcjGfOXvOsP6LHbQeDnafbYVnQjvarByDSLh67DJ4onmHvRDAQ1Pt5SCEW6T8Zuqy7Xj5ujYRteGVmf4OQp/oxJXwJyUFXDpavlzlE11OBjIT/XDw6szsiFbctMKRQvMQSytD+MUK3ymSsZjIcKYouu8JNdHYaW5EnLl6yFJWqa0OPJ9P6BaPIxrOf+rnoIFSGB/x4drIGDmteOCbSLJ+1xGcOB3+jFKZM07L0UaIGNP9OGvxGyRwOWWRB+WGlczdmB+UXOT1g+V00Tm899vmoGmRSoOPpRosTpBzB7x2cv28VruiZzgZOHa++UI+8t2K2HrrivZSE2riSvhJI4FLyeZ9x5CWWiqiPZG3hwwq7v0FIwy+hRMOC3TG21GYXcsYMzfb+fCckSCuhH/LvuOG4YlyqeNr29cNl0mmxFhDwRbXvGtcLjpeiOXMUya6+POvQ2hX39cCxgDizMdvlXW7/CuiZRe/dX/6mj2mFRcZdxzwMI+ASWzC1VhKSOHXG1RazVKLg1Dk7HVe8fCYhYgZuyj9jfEWasgwjHsSUvitoi67oEffN+ZhQc5+bLA4ALWSaB/hiGGY+IOF3wA7JYK37D9mezBsv9hkZ/BxhmESDhZ+j4imTlq3hekYholvWPgZhmESDBZ+j4iBUjUMwzAAWPg948AxDuljGCY2YOH3iLfmaA+gwjAME22w8DMMwyQYLPwMwzAJhmPhJ6JmRLRS8XeEiEaolulBRAWKZZ52bzLDMAzjBsdF2oQQ2QDaAgARJQPYCeB7jUXnCyEud7odhmEYxlu8cvX0BrBZCLHNo/UxDMMwPuGV8A8GMEVnXmciWkVEPxNRK70VENFwIlpGRMv27Yut2tYMwzCxhGvhJ6LSAK4E8LXG7BUAGgghLgTwNoD/6q1HCDFRCJEphMhMT093axbDMAyjgxct/gEAVggh9qpnCCGOCCGOSZ+nA0ghouoebJNhGIZxiBfCPwQ6bh4iqkUUKGZARB2l7Xk7iC3DMAxjC1dDLxJROQB9AdypmHYXAAghJgC4DsDdRFQE4CSAwSLWRiVmGIaJM1wJvxDiBIBqqmkTFJ/HARjnZhsMwzCMt3DmLsMwTILBws8wDJNgsPAzDMMkGCz8DMMwCQYLP8MwTILBws8wDJNgsPAzDMMkGCz8DMMwCQYLP8MwTILBws8wDJNgsPAzDMMkGCz8DMMwCQYLP8MwTILBws8wDJNgxJXwlykVV7vDMAzjC3GllCP6nB9pExiGYaKeuBL+CqmuxpVhGIZJCOJK+Hs2S4+0CQzDMFGPa+EnojwiWkNEK4lomcZ8IqKxRJRLRKuJqL3bbepRt0o5v1bNMAwTN3jlG+kphNivM28AgKbS38UAxkv/GYZhmAgQDlfPVQA+EQH+AFCZiGqHYbsMwzCMBl4IvwAwi4iWE9FwjfnnAdiu+L5DmsYwDMNEAC9cPV2EELuIqAaALCLaKISYp5hPGr8R6gnSQ2M4ANSvX98DsxiGYRgtXLf4hRC7pP/5AL4H0FG1yA4A9RTf6wLYpbGeiUKITCFEZno6R+cwDMP4hSvhJ6LyRJQmfwbQD8Ba1WI/ArhFiu7pBKBACLHbzXYZhmEY57h19dQE8D0Ryev6Qggxg4juAgAhxAQA0wEMBJAL4ASA211uk2EYhnGBK+EXQmwBcKHG9AmKzwLAvW62wyQm2S/0R7OnZkTaDIaJO+Iqc5eJL5JIKy4gtsl6sFukTWAYFv5EIymGtNRvU69tX9fnLYRyLiSejWHCDwt/gpEcS8qvQ96YQZE2wTEiNJKZYcIOC3+C0at5jbBvc0jH6MzLiIQnqUyp5PBvNAqpW6VspE2wTY84KgLJwh+jOG31/v3iBh5bYs4/uzZ09DuKQx9/w+rlI21CVBCNjYEbM+sZzr+lc/jvHb9g4feRa9pFvjJF/1a1gr5f2qR62G1olF4BK/7dN+zbjTbqVY29Vm6keO/mDmHfZsN044dySnL8yGX87AmjSdfzS4S+dqXUCFoSzL96NYm0CUwECUR569PYRIT94JyJTfEEC7+PRIOjom+LmkHfI2WTk+0a/aZPixp4d6hvQzsEccF5lRz9LhL9KX5ybfu6uKdHY935I/o0xaA2sVt4N4F0P/6E/8J6lQEAj/Zv5tk6Nz7fH/++vKX9H0aD8keDDRq4NeuDWy/CwAuMRcYr4Z16ZydHv6tfNXhgILXbzQ7T7+/q+Lde8UDvpni0f3Pd+QNa10btitbeKlP4FKO1AAAWfklEQVSSk9C8VhoeH9AcH95+UdA8M1+7X5QrHeh4H3ZpQzw+QHs/29WvHE6TfCPuhP/Lf3bC/Ed7omyKd9ETqSnJGHZpQ3RqVNXW7+pW9tena8V1Q9Gq/Baw27er7DBc9XQ/DO/WCADQpEYF7fU7tswZ6geBHlrRI7HQKVyzYpmQaXoPLCJgxohuuLN7Y/RsFvyAfvm6NohEi+WmTg3w+IDmeKx/c9zZvTHaq0SeQI7eClqfV9EjC70j7oS/bOlk1KtazrPXtrXPXlb82a6I1qiYijWj+nljiAZz/68HZj9knAmqFk+rYpqmMXD9oDa18WCf8725kH2I2FH2G1Qql1J8DVQtX9rVep02Is5TPfitRlTd3iU0Csrt4ZJbs27Qs+HihlWx5cWBqFyudEiWQss6/ole81ppnq4vJTkJd3ZvjNKlArI4ZXjwm57THIwHep8f9N2Lc+GWuBN+Ga86aiqU0S5nJF8cZqSlpnhihxapKckor2OfjJFeXN22ju48razWymVT8ECfpujbwrnLwk/q6LxhuX3EEBFGXWHN1ZeSXLK1Oy4NFnCryXN6HZ+yG9MOz1/dGque7oclT/ax/Vur1K1SDkk2EwPNGlFWHnQzRnTDp8M6Ys2ofp4/BADvci76tqwZVKrD6FyUClOCZdwKv1P+ZhCCmaQ4WqWTk/DW4Laay8kPBfXt+8U/wz/UsFEsvFJI1PtyZ/dGntrh5DHsNo7fyyzZ27o01MydUPf9KOsLOc2S1rP627s6215XaqkkVCqXotuAMWPQBbVD3lzka+PmTg0wfmh7PH91q5DfPTWohasM6+oVQt1GWnRtmo601BQMMunviTSN00vcjUbnIlz9y3Er/EYtfiPf+P9dpt8p3LlRtaDvF2Vo+/yr67gWLmkc/hh6tfToielVbUseeI2ql0ftSv71T8x5uHtYPLgZ1QJ+ca8jTZRvSg0s+u2NaFBNtQ6dS7eUgzhyuw/PV68PLrb72g0hxXeL81MGtamNARfURrnS9h8q6n4MtTutUtkU5IwegDs03F5ahCvXz6kwy/Zd3NC4nzBcIaVxLPza07+9+xLMGNENb96o3VovY+DCuadHk+LOKiGEue83zPFhXZsGP1jKpiQbVrgkADNGdA3pJ5jmYwRJ5XIpaJxeISwtmzqVy2Lj8/1xcyfvMi7zxgzCm4PbFX9XH94Wtc192ko3Yd6YQejUMLhBofWm4lTY7P6s+/nBgpyquMZlG5rXqhiwW9UQ0qNPi9DoqqY1g10zS58KdX+kJCfh6StaYuAF5q7FcNxqQjjfEBFh5ohumHTbReYLh4E4Fn7tE1SvSllUKpuCq9udh8m3ZdpaZ1ISob7UOhMAqui07MNZasD0OjQxpXmtimhSI/gmLGuz80kv9M0qRqnwbw1uiwWP9URPh3VSUlOSQUS4sG5oLL7eG5vMxQ2r4qPbtW9ULXu+urMzPr5dPfJoKAse7Wk430sRs3Mp5o4egPS0MiHRaxnVA9e81X4tNff1ahr0vVnNUH+87P6oqBFUYCWookNGleLPevd15XKh/W2ZDapoLKkNkbs+u2a10kxdbuFqK8at8Fs5gL2a1zRfSIUdSQ93PojWPofjGZSeZu6PDTFDYaxy3tMqn/lVbc9D3Srl8KGOoI7o01Rzupr3bs7Eg32CoyvqVi1r6IduXisNPZpZzwXo2LAqKmmIi5oaJrHuWg9eOxFlTwxsrtlxb5aIJruS1FFQ7/69Aybdmokaaebhw1rXoLqz+k2dvrFv7uqMrIe6h0zvbuGhr3TDtqpTSfOafLhfqBv3rSHtQqbpkUSEN25sizu7N3LdB/bVnZ3x1Z0lfTbh6tSViWPh15ZdMzGW51ctX9pQFIweLHILqZbFZBY/cXM5LRzZCyv+3Re9VYlQXjxM9A6fOhLGjBGSmA+92LjoV61KqXhA9ZCQxfT8mtpx/lbw+sF6T4/G6NyomquQ2eHdGhe7aJT1ZT4d1tEweEGPSuVS0LuF/UaSHuV1+gQyM6qipsY9c0NmPTw1qIXl9WutAwgdi+KSxtVQQ3pAWBFeQqCR8/iAFnh8gDV71B3jMh0bVkVHhb//vDBXK3Us/ERUj4jmEtEGIlpHRA9oLNODiAqIaKX097Q7c62j1wp1+ypl5Ua/7ZKG+GzYxejb0rubRQ87uyObLr/eNqtlLC51KpdF1fKl0cMkA9bJMfVSSPLGDMLoay6w/Ts3oq3c5TqVUoPcCLMf6oY/Hu/taL0P9GkKIsJnwy7Gt3eXtAjt2vrEoBb4V68mQdnNlcuVxsvXtcHIAc0x7f5LdV03fiX91aqYimeuaFnsLrWDuu9BjRX3qtpNQ+ROD2Y92C3kLVJN9QrWckhk68NV2NFNi78IwMNCiBYAOgG4l4i0gp3nCyHaSn/PudieLa7vUA8Tbgqt5aLl51NidvmklkpGy9oV8caNodEOMkkEXNq0etjLCmt3Coba8M3dl+D3kb3QubG1zjknd0elssHHWc43kLNp2zqISfca2dfstjGw4LFeWPFUSfXRJjXSUEuKHFOXI9BCPm9j/nZBcex45XKl0aGBtUzxh/uGik/F1BQ83K9ZSEhpSnIS7ureGK3qVMKmFwagjlaEm4vL1uiSr1kpVTM5zQrycbFb4XTxE72LM7db1ErD+7do+/+rWRFo1b6dXzMNtSpZCzs1o3Pjalj1TD+8cl0bT9ZnhmPhF0LsFkKskD4fBbABQOTrEEskJRH6t64dEhGQahKJY6YBSUmE6Q90Rf/W/sYNP3JZs6BXQT3MqhzqofcKagUruvDbIz2CvpculYS8MYNwT4/oqcopd87r1Z+5xuLQjElJpJvApC5HUPwbxeLyKbTSTmhVpyKuuDDYf39fryaOY+ZvvCjUReamuaJ1OTarlYa6VcpipEGdH78gBN8j6rfw0qWS8PK1F+Cbuy7xyQBrR/PZK1ujUtkURyG7TvBkK0SUAaAdgMUaszsT0Soi+pmIQjM9fEYpcLE06s+9PZvgpb+Zuy/MdF952Tl+AzH5nTqM1C8+/8fFIQ8UNWZvdC01wi31XHJW3kqcukXmP9YL36gSsozWJc+Zdn9XPHtl8G2kPK8dTSKV1AxqExoqqUw28oJypUthwWM23jANsNrOsRLoIC9z40X1UU8nH6ObwsWkdX66NjV2Qd3RJcNwvozTiCmnOEvnU0BEFQB8C2CEEOKIavYKAA2EEMeIaCCA/wLQDMMgouEAhgNA/frejc6jjOfXi7EedmlDTFqw1bNtqnnmipaoKPkXb+ncAJ8s2mbpd3pvJ3pFx9SMdBlmKSNXXJQTotSYRal4RRcLg8jMf7QnThWd053/3T2XoPm/Z7i2xa176LzKZYsbJXZXVUoqC9G8Vhq+Vjw8Fj3eC5XL2qtLpLUf9/duirfm5Ni0yl/kfa5cLgU7Dp20/kONNys7KKU+JNEO+mVCAGuj5M17pGdExmF2JfxElIKA6H8uhPhOPV/5IBBCTCeid4mouhBiv8ayEwFMBIDMzEzPjkTXptWLRV3rxMsn554ejYMEw8vXXaVf87mrWlsWfi0b1j93mW4pAPV2OzSo4knUSZ+WNfHZsItxicUW24SbOth+u7iwbiUcP33WiXlBpKWmwKhqS2pKMt4d2h6pKcEtrGY105C99ygA4ME+5+P46SLXttjG4iGrmJqC8UPbIzOjalCHpZtsa2X8vNNSEwBw+YW1Mfn3raadsXapU7ksnruqFfq1rIVOL83xZJ1WLtHu56fjt037MP3+rroiP6RjPUxZst2RDU46ur3AsfBT4M6eBGCDEOJ1nWVqAdgrhBBE1BEB19IBp9t0Qo9mNTB2SDvcP+VPwxNdTaoNsv/YKdfb9OqpVbFsqNtCnR5v7upxXzsGCHRWF6/TYDUP9G6Ky1rVxJFCe8L5w32XFn9OTiJcZ9G/7gStOv5fDu+Eds9nAUBI2KcWXvbbO2mJDvCoNo28aa/e2trXr+KqRo8Rt3TOsLG0CPnk5Jzd3iUDV7WtU6wPRtx2SQY+WphnfyMRwE2LvwuAmwGsIaKV0rQnANQHACHEBADXAbibiIoAnAQwWDjtjXSBURmGaGPgBbXw5o2BpJIKZUphzah+uGDULN3lla+Jwy5tiIWbS56rDauXDyosd9slGZ7YWLlcqDth4s0dUK1CGXSQQkXdCOPmFwc6/7FD5I7e8hazlr28iuVzaFhew+cIsdgdtcGYYL984HO50sk4cfosHuprPlgTEZmK/m2XNMT/Vu9G7xY14l/4hRALYHK9CCHGARjndBteY+Vm9eKGdnoTDbqgNsYOaRfUMle+yhule9evWi4kNl4Oofx0WEe0qlPJdV16mSEd6+OcEHj6h3XF0/qpRpeKxWHs5j3SExU0SgZokVGtHH6DeWeyJeSoHo1ZbmPNzZC3afTQiSXkImgdGlTBzHV7USYlKeQVvEezdExfs8dyX5kZzWqlYc2oy5C956gn6wsHrjt3YwEnl7Sb+8Dub2/t3AAfL9qG1JRkQ3eM1hwromAWeWCX5CTCDZn1goRfjzSH5YAjgR1/6xODWqBHsxpoV9+81svoa1obljuI5DOySY0K+GfXhhhqcZCYaGb2Q92LK+++eWM7bN1/HBVTUzCsa0M8+f3a4hHCXr+hLe7reTwk18QtkeikdUrs3JVhxmorKz2tDPYdDe4XsCP8d3RpWDyIhN7vUpIJZ84al6AId4PNtG8hPhqQupQplYyeFsf0NRNVebQzsxwTPyAiPDnIwXjSEaRfy5qax17Zgi9bOrl49K+hFzcIOgepKcm+jAwWS2+5LPwu0Xw9t/mOUeLj1Z6/7Mm+uPC5WYavLnGus3HNY/2bo07lshjQOjpHNos2Jupk30aalDAlX3lB7FgaJuRiTbUrW4ty0GzZ2lThs1IUqZmfVdvVE0PNDBVedTbHOuXLlMJd3RtrZv/yAz12aFKjAkZf0zrSZlgiIVr8GdUDiUdW4tCrlC+Ntwa3dTValp2btValMmhXP5Ah2q+VdgZpuTIBF8AjGinvJa6e2JMIrYQYJpgaaanYc6QwItuO9uEMo5GhFzdA+dKloj6SMCGE//yaaVj8RO/iEqxmKIchNEPLrdOqjnHdc5l3h7bHZa1qITmJsOXFgbr1XlKSk3Rjo+tWKYuMauUw6sqwV8MwJEWKI21nY6ALJpRv7u6MJVsPusrBcEL2C/1RKim6xStauTpMFTbdkBDCD+jX6HZLxbKlsEdVqMLKwCRAcCKRnuibUaZUMn59xHhEp0hQtnQy/vevS9GwunaZB4DdGFaoW6Uc6lYJ/5uRXA2TiU/4ke6SD26JjjE0w42V0LXW51UqziXQXgfDMJGAhd8lVjuBGYZhooWEcfWECysDuH9/zyWobqH2RzTjxShN7OphmMjAwu8S5VidVotTWcn2jATVK5Q2LGmsJJayFBmGCYaF3yVEhEWP98JJD0oKR5pFj/cOa/Zhz+Y1MOqn9bi2g3+VOBmGCYV9/B5Qu1JZNPJ41CKnvHp9YCzg6xyIaUpyUlhHAmpQrTzyxgxCm7qRH3+XYRIJbvHHGdd1qItqFUqjq4XRqtzA4X4ME7uw8MchegN8e0lyEmH80PYRKSzGMIw7WPgZx3g1ChTDMOGFffwMwzAJBgs/wzBMguFK+ImoPxFlE1EuEY3UmF+GiKZK8xcTUYab7TEMwzDucSz8RJQM4B0AAwC0BDCEiNRD+QwDcEgI0QTAGwBedro9hmEYxhvctPg7AsgVQmwRQpwG8CWAq1TLXAXgY+nzNwB6UywWjmcYhokj3Aj/eQC2K77vkKZpLiOEKAJQAMB8NBSGYRjGN9wIv+ZIgA6WCSxINJyIlhHRsn379rkwi2EYhjHCjfDvAFBP8b0ugF16yxBRKQCVABzUWpkQYqIQIlMIkZmenu7CLIZhGMYINwlcSwE0JaKGAHYCGAzg76plfgRwK4BFAK4D8IuwMDr48uXL9xPRNod2VQew3+FvowG2P7Kw/ZGF7XdOA6sLOhZ+IUQREd0HYCaAZACThRDriOg5AMuEED8CmATgUyLKRaClP9jiuh03+YlomRDCvCh+lML2Rxa2P7Kw/eHBVckGIcR0ANNV055WfC4EcL2bbTAMwzDewpm7DMMwCUY8Cv/ESBvgErY/srD9kYXtDwNkoa+VYRiGiSPiscXPMAzDGBA3wm9WMC6SEFEeEa0hopVEtEyaVpWIsogoR/pfRZpORDRW2o/VRNResZ5bpeVziOhWH+2dTET5RLRWMc0ze4mog3Q8cqXfelrGQ8f+UUS0UzoHK4looGLe45It2UR0mWK65jVFRA2looM5UhHC0h7bX4+I5hLRBiJaR0QPSNNj4hwY2B8T54CIUoloCRGtkux/1mibZFCM0u5+hQ0hRMz/IRBOuhlAIwClAawC0DLSdinsywNQXTXtPwBGSp9HAnhZ+jwQwM8IZD13ArBYml4VwBbpfxXpcxWf7O0GoD2AtX7YC2AJgM7Sb34GMCAM9o8C8H8ay7aUrpcyABpK11Gy0TUF4CsAg6XPEwDc7bH9tQG0lz6nAdgk2RkT58DA/pg4B9IxqSB9TgGwWDqumtsEcA+ACdLnwQCmOt2vcP3FS4vfSsG4aENZwO5jAFcrpn8iAvwBoDIR1QZwGYAsIcRBIcQhAFkA+vthmBBiHkIzrD2xV5pXUQixSATujk8U6/LTfj2uAvClEOKUEGIrgFwErifNa0pqGfdCoOggEHwsvLJ/txBihfT5KIANCNS9iolzYGC/HlF1DqTjeEz6miL9CYNt6hWjtLVfXtlvhXgRfisF4yKJADCLiJYT0XBpWk0hxG4gcKMAkAfK1duXSO+jV/aeJ31WTw8H90mukMmymwT27a8G4LAIFB1UTvcFyW3QDoFWZ8ydA5X9QIycAyJKJqKVAPIReGBuNtimXjHKaL2X40b4LReDixBdhBDtERi74F4i6mawrN6+ROs+2rU3UvsxHkBjAG0B7AbwmjQ9au0nogoAvgUwQghxxGhRHZsiug8a9sfMORBCnBVCtEWgBllHAC0Mthl19psRL8JvpWBcxBBC7JL+5wP4HoELaa/0yg3pf760uN6+RHofvbJ3h/RZPd1XhBB7pZv5HID3ETgHMLFTa/p+BFwppVTTPYWIUhAQzc+FEN9Jk2PmHGjZH2vnQLL5MIBfEfDx621TrxhltN7LcdO5WwqBjquGKOksaRVpuyTbygNIU3xeiIBv/hUEd9T9R/o8CMEddUuk6VUBbEWgk66K9Lmqj3ZnILhz1DN7ESjw1wklHYsDw2B/bcXnBxHwvQJAKwR3wG1BoPNN95oC8DWCO/nu8dh2QsDv/qZqekycAwP7Y+IcAEgHUFn6XBbAfACX620TwL0I7tz9yul+hesvbBvyfUcCkQ2bEPDFPRlpexR2NZJO7CoA62TbEPABzgGQI/2Xb0hCYEjLzQDWAMhUrOsOBDqIcgHc7qPNUxB4FT+DQOtkmJf2AsgEsFb6zThIiYQ+2/+pZN9qBKrGKkXoScmWbCiiW/SuKemcLpH262sAZTy2/1IEXv1XA1gp/Q2MlXNgYH9MnAMAbQD8Kdm5FsDTRtsEkCp9z5XmN3K6X+H648xdhmGYBCNefPwMwzCMRVj4GYZhEgwWfoZhmASDhZ9hGCbBYOFnGIZJMFj4GYZhEgwWfoZhmASDhZ9hGCbB+H8yVVCC/KPP4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_values), len(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(v_rnn.state_dict(), 'models/v_rnn.pkl')\n",
    "v_rnn.load_state_dict(torch.load('models/v_rnn.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre trained wikitext 103 model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text.learner import *\n",
    "# from fastai.datasets import URLs\n",
    "# from fastai.basic_data import DataBunch\n",
    "\n",
    "# seq_len = 10\n",
    "# bible_df = DataLoader(bible_dataset(seq_len), 1)\n",
    "# language_model_learner(data=DataBunch(bible_df,bible_df,bible_df), arch=AWD_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the model weights and vocab\n",
    "# ! wget http://files.fast.ai/models/wt103_tiny.tgz -P \"C:\\\\Git\\\\karpathy-rnn\"\n",
    "# ! tar xf \"C:/Git/karpathy-rnn/models/wt103_tiny.tgz\" -C \"C:/Git/karpathy-rnn/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "awd_lstm_model =  get_language_model(arch=AWD_LSTM, vocab_sz=len(bible_df))\n",
    "old_wgts  = torch.load('models/pretrained/lstm_wt103.pth', map_location='cpu')\n",
    "old_vocab = pickle.load(open('models/pretrained/itos_wt103.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.encoder.weight', '0.encoder_dp.emb.weight', '0.rnns.0.weight_hh_l0_raw', '0.rnns.0.module.weight_ih_l0', '0.rnns.0.module.weight_hh_l0', '0.rnns.0.module.bias_ih_l0', '0.rnns.0.module.bias_hh_l0', '0.rnns.1.weight_hh_l0_raw', '0.rnns.1.module.weight_ih_l0', '0.rnns.1.module.weight_hh_l0', '0.rnns.1.module.bias_ih_l0', '0.rnns.1.module.bias_hh_l0', '0.rnns.2.weight_hh_l0_raw', '0.rnns.2.module.weight_ih_l0', '0.rnns.2.module.weight_hh_l0', '0.rnns.2.module.bias_ih_l0', '0.rnns.2.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_wgts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained vocab and bible vocab need to be matched up where there's overlap\n",
    "idx_the_new, idx_the_old = word_to_ix['the'],old_vocab.index('the')\n",
    "\n",
    "the_wgt  = old_wgts['0.encoder.weight'][idx_the_old]\n",
    "the_bias = old_wgts['1.decoder.bias'][idx_the_old] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match overlap \n",
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    wgts = old_wgts['0.encoder.weight']\n",
    "    bias = old_wgts['1.decoder.bias']\n",
    "    wgts_m,bias_m = wgts.mean(dim=0),bias.mean()\n",
    "    new_wgts = wgts.new_zeros(len(word_to_ix), wgts.size(1))\n",
    "    new_bias = bias.new_zeros(len(word_to_ix))\n",
    "    otoi = {v:k for k,v in enumerate(old_vocab)}\n",
    "    for i,w in enumerate(word_to_ix): \n",
    "        if w in otoi:\n",
    "            idx = otoi[w]\n",
    "            new_wgts[i],new_bias[i] = wgts[idx],bias[idx]\n",
    "        else: new_wgts[i],new_bias[i] = wgts_m,bias_m\n",
    "    old_wgts['0.encoder.weight']    = new_wgts\n",
    "    old_wgts['0.encoder_dp.emb.weight'] = new_wgts\n",
    "    old_wgts['1.decoder.weight']    = new_wgts\n",
    "    old_wgts['1.decoder.bias']      = new_bias\n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_to_ix)\n",
    "wgts = match_embeds(old_wgts, old_vocab, word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.6640), tensor(2.6640))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that the weights and bias are equal\n",
    "wgts['0.encoder.weight'][idx_the_new],the_wgt\n",
    "wgts['1.decoder.bias'][idx_the_new],the_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vanilla_rnn(\n",
       "  (rnn): RNN(31402, 12, batch_first=True)\n",
       "  (fc): Linear(in_features=12, out_features=31402, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(31402, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(31402, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=31402, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.encoder.weight', '0.encoder_dp.emb.weight', '0.rnns.0.weight_hh_l0_raw', '0.rnns.0.module.weight_ih_l0', '0.rnns.0.module.weight_hh_l0', '0.rnns.0.module.bias_ih_l0', '0.rnns.0.module.bias_hh_l0', '0.rnns.1.weight_hh_l0_raw', '0.rnns.1.module.weight_ih_l0', '0.rnns.1.module.weight_hh_l0', '0.rnns.1.module.bias_ih_l0', '0.rnns.1.module.bias_hh_l0', '0.rnns.2.weight_hh_l0_raw', '0.rnns.2.module.weight_ih_l0', '0.rnns.2.module.weight_hh_l0', '0.rnns.2.module.bias_ih_l0', '0.rnns.2.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([31402, 400]), torch.Size([31402, 400]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['0.encoder.weight'].shape,old_wgts['0.encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31402])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_model\n",
    "awd_lstm_model.state_dict()['1.decoder.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(awd_lstm_model.state_dict().keys()),len(wgts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SequentialRNN:\n\tsize mismatch for 0.rnns.0.weight_hh_l0_raw: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.0.module.weight_ih_l0: copying a param with shape torch.Size([4600, 400]) from checkpoint, the shape in current model is torch.Size([4608, 400]).\n\tsize mismatch for 0.rnns.0.module.weight_hh_l0: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.0.module.bias_ih_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.0.module.bias_hh_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.1.weight_hh_l0_raw: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.1.module.weight_ih_l0: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.1.module.weight_hh_l0: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.1.module.bias_ih_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.1.module.bias_hh_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.2.module.weight_ih_l0: copying a param with shape torch.Size([1600, 1150]) from checkpoint, the shape in current model is torch.Size([1600, 1152]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1fe6c1b0961e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#load the pretrained weights into the model architecture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mawd_lstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwgts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 839\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    840\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SequentialRNN:\n\tsize mismatch for 0.rnns.0.weight_hh_l0_raw: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.0.module.weight_ih_l0: copying a param with shape torch.Size([4600, 400]) from checkpoint, the shape in current model is torch.Size([4608, 400]).\n\tsize mismatch for 0.rnns.0.module.weight_hh_l0: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.0.module.bias_ih_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.0.module.bias_hh_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.1.weight_hh_l0_raw: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.1.module.weight_ih_l0: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.1.module.weight_hh_l0: copying a param with shape torch.Size([4600, 1150]) from checkpoint, the shape in current model is torch.Size([4608, 1152]).\n\tsize mismatch for 0.rnns.1.module.bias_ih_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.1.module.bias_hh_l0: copying a param with shape torch.Size([4600]) from checkpoint, the shape in current model is torch.Size([4608]).\n\tsize mismatch for 0.rnns.2.module.weight_ih_l0: copying a param with shape torch.Size([1600, 1150]) from checkpoint, the shape in current model is torch.Size([1600, 1152])."
     ]
    }
   ],
   "source": [
    "#load the pretrained weights into the model architecture\n",
    "awd_lstm_model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x000001848BA8C048>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.encoder.weight : (31402, 400)\n",
      "0.rnns.0.weight_hh_l0_raw : (4608, 1152)\n",
      "0.rnns.0.module.weight_ih_l0 : (4608, 400)\n",
      "0.rnns.0.module.weight_hh_l0 : (4608, 1152)\n",
      "0.rnns.0.module.bias_ih_l0 : (4608,)\n",
      "0.rnns.0.module.bias_hh_l0 : (4608,)\n",
      "0.rnns.1.weight_hh_l0_raw : (4608, 1152)\n",
      "0.rnns.1.module.weight_ih_l0 : (4608, 1152)\n",
      "0.rnns.1.module.weight_hh_l0 : (4608, 1152)\n",
      "0.rnns.1.module.bias_ih_l0 : (4608,)\n",
      "0.rnns.1.module.bias_hh_l0 : (4608,)\n",
      "0.rnns.2.weight_hh_l0_raw : (1600, 400)\n",
      "0.rnns.2.module.weight_ih_l0 : (1600, 1152)\n",
      "0.rnns.2.module.weight_hh_l0 : (1600, 400)\n",
      "0.rnns.2.module.bias_ih_l0 : (1600,)\n",
      "0.rnns.2.module.bias_hh_l0 : (1600,)\n",
      "1.decoder.bias : (31402,)\n"
     ]
    }
   ],
   "source": [
    "for l in list(awd_lstm_model.named_parameters()):\n",
    "    print(l[0], ':', l[1].detach().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.encoder.weight: torch.Size([31402, 400])\n",
      "0.encoder_dp.emb.weight: torch.Size([31402, 400])\n",
      "0.rnns.0.weight_hh_l0_raw: torch.Size([4600, 1150])\n",
      "0.rnns.0.module.weight_ih_l0: torch.Size([4600, 400])\n",
      "0.rnns.0.module.weight_hh_l0: torch.Size([4600, 1150])\n",
      "0.rnns.0.module.bias_ih_l0: torch.Size([4600])\n",
      "0.rnns.0.module.bias_hh_l0: torch.Size([4600])\n",
      "0.rnns.1.weight_hh_l0_raw: torch.Size([4600, 1150])\n",
      "0.rnns.1.module.weight_ih_l0: torch.Size([4600, 1150])\n",
      "0.rnns.1.module.weight_hh_l0: torch.Size([4600, 1150])\n",
      "0.rnns.1.module.bias_ih_l0: torch.Size([4600])\n",
      "0.rnns.1.module.bias_hh_l0: torch.Size([4600])\n",
      "0.rnns.2.weight_hh_l0_raw: torch.Size([1600, 400])\n",
      "0.rnns.2.module.weight_ih_l0: torch.Size([1600, 1150])\n",
      "0.rnns.2.module.weight_hh_l0: torch.Size([1600, 400])\n",
      "0.rnns.2.module.bias_ih_l0: torch.Size([1600])\n",
      "0.rnns.2.module.bias_hh_l0: torch.Size([1600])\n",
      "1.decoder.weight: torch.Size([31402, 400])\n",
      "1.decoder.bias: torch.Size([31402])\n"
     ]
    }
   ],
   "source": [
    "for l in list(old_wgts.keys()):\n",
    "    print(f'{l}: {old_wgts[l].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(31402, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(31402, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=31402, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_clas_config = dict(emb_sz=400, n_hid=1150, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.4,\n",
    "                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5, tie_weights=None, out_bias=1)\n",
    "awd_lstm_model_new = get_language_model(arch=AWD_LSTM,config=awd_lstm_clas_config, vocab_sz=len(bible_df))\n",
    "awd_lstm_model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(awd_lstm_model_new.state_dict().keys()),len(wgts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the pretrained weights into the model architecture\n",
    "awd_lstm_model_new.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Then we split by doing two groups for each rnn/corresponding dropout, \n",
    "# #then one last group that contains the embeddings/decoder. This is the one that needs to be trained the most as we may have new embeddings vectors.\n",
    "# def lm_splitter(m):\n",
    "#     groups = []\n",
    "#     for i in range(len(m[0].rnns)):\n",
    "#         groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n",
    "#     groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]\n",
    "#     return [list(o.parameters()) for o in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(31402, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(31402, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=31402, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we train with the RNNs freezed.\n",
    "for rnn in awd_lstm_model_new[0].rnns:\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad_(False)\n",
    "#         print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(31402, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(31402, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=31402, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awd_lstm_model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.769428253173828\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-276-7a9735cb6c33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mawd_lstm_model_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#setup the dataloader\n",
    "seq_len = 10\n",
    "bible_df = DataLoader(bible_dataset(seq_len), 1)\n",
    "#reset the model\n",
    "awd_lstm_model_new.reset()\n",
    "#define hyperparams\n",
    "# n_epochs = 3\n",
    "lr=3e-6\n",
    "optimizer = torch.optim.Adam(awd_lstm_model_new.parameters(), lr=lr)\n",
    "iterations_per_epoch = len(bible_df)\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/10))\n",
    "\n",
    "#loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "epoch_loss = []\n",
    "#training loop\n",
    "for epoch in range(1,4):\n",
    "    loss_values = []\n",
    "    loss= 0.\n",
    "    for local_batch in enumerate(bible_df, 0):\n",
    "        input_seq = local_batch[1][0].cuda()\n",
    "        target_seq = local_batch[1][1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden1, hidden2 = awd_lstm_model_new(input_seq.long())\n",
    "        a = awd_lstm_model_new.state_dict()['1.decoder.weight'][input_seq].clone()\n",
    "        loss = loss_func(input=output.squeeze(-3), target=target_seq.squeeze(-2).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        b = awd_lstm_model_new.state_dict()['1.decoder.weight'][input_seq].clone()\n",
    "        #check if it's trained. if it hasn't then stop the training\n",
    "        assert torch.equal(b.data, a.data)==False\n",
    "        loss_values.append(loss.data.item())\n",
    "        if (len(loss_values) % 100 == 0) | (len(loss_values) <= 100):\n",
    "            print(loss.data.item())\n",
    "            loss_values.append(loss.data.item())\n",
    "    #epoch avg loss\n",
    "    epoch_loss.append(sum(loss_values)/len(loss_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(awd_lstm_model_new.state_dict(), 'pretrained_frozen.pth')\n",
    "pickle.dump(vocab, open('vocab_pretrained_frozen.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the whole model now (unfreeze the pretrained layers)\n",
    "for rnn in awd_lstm_model_new[0].rnns:\n",
    "    for p in awd_lstm_model_new.parameters():\n",
    "      p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the dataloader\n",
    "seq_len = 10\n",
    "bible_df = DataLoader(bible_dataset(seq_len), 1)\n",
    "\n",
    "#define hyperparams\n",
    "n_epochs = 1\n",
    "lr=5e-1\n",
    "optimizer = torch.optim.Adam(v_rnn.parameters(), lr=lr)\n",
    "iterations_per_epoch = len(bible_df)\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 3, eta_min=lr/10))\n",
    "\n",
    "#loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "#reset the model\n",
    "awd_lstm_model_new.reset()\n",
    "\n",
    "epoch_loss = []\n",
    "#training loop\n",
    "for epoch in range(1,3):\n",
    "    loss_values = []\n",
    "    loss= 0.\n",
    "    for local_batch in enumerate(bible_df, 0):\n",
    "        input_seq = local_batch[1][0].cuda()\n",
    "        target_seq = local_batch[1][1].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden1, hidden2 = awd_lstm_model_new(input_seq.long())\n",
    "        loss = loss_func(input=output.squeeze(-3), target=target_seq.squeeze(-2).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_values.append(loss.data.item())\n",
    "        if (len(loss_values) % 100 == 0) | (len(loss_values) <= 100):\n",
    "            print(loss.data.item())\n",
    "            loss_values.append(loss.data.item())\n",
    "    #epoch avg loss\n",
    "    epoch_loss.append(sum(loss_values)/len(loss_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(awd_lstm_model_new.state_dict(), 'pretrained_unfrozen.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
